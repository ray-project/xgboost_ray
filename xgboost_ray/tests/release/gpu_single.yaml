cluster_name: xgboost_ray_release_tests_gpu_single
min_workers: 0
max_workers: 0
initial_workers: 0
autoscaling_mode: default
docker:
    image: "rayproject/ray:latest-gpu"
    container_name: ray_container
    pull_before_run: false
    run_options:
        - --privileged
target_utilization_fraction: 0.8
idle_timeout_minutes: 5
provider:
    type: aws
    region: us-west-2
    availability_zone: us-west-2a
    cache_stopped_nodes: true
auth:
    ssh_user: ubuntu
head_node:
    InstanceType: p2.xlarge
    ImageId: ami-0a2363a9cff180a64 # Deep Learning AMI (Ubuntu) Version 30

worker_nodes:
    InstanceType: p2.xlarge
    ImageId: ami-0a2363a9cff180a64 # Deep Learning AMI (Ubuntu) Version 30

file_mounts: {
  "/release_tests": "./"
}
cluster_synced_files: []
file_mounts_sync_continuously: true
initialization_commands: []
setup_commands:
  - pip install -U ray
  - pip install -U git+https://github.com/ray-project/xgboost_ray@master#xgboost-ray
  - pip install -U pyarrow
  - pip install -U cupy-cuda101
  - mkdir -p /data
  - apt update && apt install -y nfs-common vim
  # /data/parted.parquet. 1.5B rows, 4 features, 2 labels
  - mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport fs-f788aaf2.efs.us-west-2.amazonaws.com:/ /data
head_setup_commands: []
#  - if ! `ls /data/lost+found`; then mount /dev/nvme0n1 /data; fi
worker_setup_commands: []
#  - if ! `ls /data/lost+found`; then mount /dev/nvme1n1 /data; fi
head_start_ray_commands:
    - ray stop
    - "ulimit -n 65536; ray start --head --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml --resources='{\"actor_cpus\": 4, \"actor_gpus\": 1}'"
worker_start_ray_commands:
    - ray stop
    - "ulimit -n 65536; ray start --address=$RAY_HEAD_IP:6379 --object-manager-port=8076 --resources='{\"actor_cpus\": 4, \"actor_gpus\": 1}'"
metadata:
    anyscale:
        working_dir: "/release_tests"
